# Recent Research by Our Visual Perception Group

## Human Motion Understanding and Generation

### 1. Human Action Understanding
#### [USDRL: Unified Skeleton-Based Dense Representation Learning with Multi-Grained Feature Decorrelation](https://arxiv.org/html/2412.09220v2), [Code](https://github.com/wengwanjiang/USDRL), [AAAI2025](https://ojs.aaai.org/index.php/AAAI/article/view/32899)

We propose a simple yet effective method named Unified Skeleton-based Dense Representation Learning (USDRL) that learns dense representations through multi-grained feature decorrelation, demonstrating the feasibility of feature decorrelation in skeleton-based dense representations learning.

#### [Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection](https://arxiv.org/html/2412.17210v2), [Code](https://github.com/guijiejie/DCMD-main), [AAAI2025](https://ojs.aaai.org/index.php/AAAI/article/view/32829)

We introduce a novel framework that seamlessly integrates reconstruction-based and prediction-based methods for video anomaly detection, leveraging the strengths of both approaches. We propose a Dual Conditioned Motion Diffusion (DCMD), which incorporates both conditioned motion and conditioned embedding in a diffusion-based model.

### [Zero-Shot Skeleton-based Action Recognition with Dual Visual-Text Alignment](https://arxiv.org/abs/2409.14336)

We propose an Dual Visual-Text Alignment (DVTA), a novel zero-shot approach for skeleton-based action recognition. The method enhances generalization to unseen classes by jointly optimizing two modules: Direct Alignment (DA) and Augmented Alignment (AA).

### [Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](https://arxiv.org/abs/2412.03044), [Code](https://github.com/Xiaofeng-Tan/FGDMAD-Code)

### [Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models](https://arxiv.org/abs/2501.13795), [Code](https://github.com/Chaolei98/FreeZAD), [Project](https://chaolei98.github.io/FreeZAD/)

To the best of our knowledge, we are the first to investigate the problem of training-free ZSTAD. We propose FreeZAD, a training-free approach for ZSTAD, which effectively leverages the generalization capabilities of ViL models to detect unseen activities. We introduce a simple yet effective TTA method that extends FreeZAD and enhances its performance by enabling adaptation to a video sequence without supervision.

### [Region-aware Image-based Human Action Retrieval with Transformers](https://arxiv.org/html/2407.09924v2), [CVIU2024](https://www.sciencedirect.com/science/article/abs/pii/S1077314224002832)

We empirically study the neglected task of image-based human action retrieval, and establish new benchmarks and important baselines to promote research in this field. We introduce an efficient Region-aware Image-based human Action Retrieval with Transformers (RIART), which leverages both person-related and contextual object cues, and employs a fusion transformer module for human action retrieval.

### 2. Human Motion Generation

# Below is the Chinese version (以下为对应的中文版本)

## 人体动作理解与生成

### 1. 人体行为理解

### 2. 人体动作生成
